% ..........................................................................
% --------------------------------------------------------------------------
% ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%              Crypto 2020
% /~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newpage
\hypertarget{Chapter_Crypto2020}{}
\chapter{Crypto 2020 --- Perspectives for Long-Term Cryptographic
         Security}\index{security!long-term}
\label{Chapter_Crypto2020}
\begin{sloppypar}
(Johannes Buchmann, Erik Dahmen, Alexander May and Ulrich Vollmer, TU Darmstadt, May~2007)\\
\end{sloppypar}

% \begin{abstract}Ever more powerful hardware and new
%   mathematical algorithms threaten to undermine
%   the security of cryptographic keys and schemes.
%   How long will the methods we use today be able
%   to keep what they promise?  And which
%   alternatives are on the horizon?
% \end{abstract}

Cryptography is a basic building block of all IT
security solutions.  Yet, for how long are the
cryptographic tools we use today going to remain
secure?  Is this time long enough to ensure the
confidentiality of medical data, to name just one
example?  Even in the short-term, the potential
for havoc is great if certain keys are broken.
Just think of the digital signatures that protect
the authenticity of automatic updates for the
Windows operating system.


\section{Widely used schemes}
\label{sec:the-weak}

In 1978, Rivest, Shamir and Adleman suggested the
RSA\index{RSA} public key encryption and signature
schemes~\cite{rivest/shamir/adleman:1978}.  RSA is
still the most widely used public key scheme.  The
security of RSA depends on the difficulty of
factoring so-called RSA moduli which are products
of two large prime numbers.  In their 1978 paper,
the inventors of RSA suggested the use of RSA
moduli with 200 decimal digits for long-term
security.  Later, the company RSA Security
published a list of RSA moduli of increasing size,
the RSA challenge numbers.  RSA Security offered
prizes totaling \$ 635,000 for the factorization
of these numbers, cf.\
\url{www.rsasecurity.com/rsalabs/}.

In 2005, that is 27 years after the invention of
RSA, Bahr, Boehm, Franke, and Kleinjung from
Bochum University managed to factor a 200 digit
RSA challenge number
(\url{www.mat.uniroma2.it/~eal/rsa640.txt}).  A
key with size originally thought to be secure for
a very long time was broken with a computation
that took them just five months.  This illustrates
the tremendous progress factoring technology has
made within the last 30 years.  This progress is
based on break-through mathematical ideas --- e.g.\
the number field sieve proposed by John
Pollard --- as well as significant developments in
computer hardware and software implementation
technology.\footnote{%
Please compare chapter \ref{SecurityRSA}
\hyperlink{SecurityRSA}{Considerations regarding the
security of the RSA algorithm}, and especially chapters
\ref{nt:NoteFactorization} and \ref{FactorizationResearch}.
}

In 2000, Lenstra and Verheul\index{Lenstra/Verheul}
developed an extrapolation formula that is supposed
to help us forecast the security\index{security!forecast}
one can achieve with RSA and other important
cryptographic schemes in the long
term (\url{www.keylength.com}).  The formula
suggests the use of 850 digit RSA moduli if one
wishes to protect data for the next 30 years.
This corresponds to a 3072 bit RSA key.

Yet, even a well thought out extrapolation formula
is no security guarantee!  At any time, a
brilliant mathematical idea can allow us to factor
large numbers easily, and destroy the security of
RSA.  In 1996, Peter Shor showed that a quantum
computer --- a new type of computer that leverages
the laws of quantum mechanics to speed up certain
types of computations --- can in principle be used
for the fast factorization of large numbers \cite{shor:1997}.
Despite intensive research in the area, it is
still too early to judge whether we are ever going
to be able to build quantum computers\index{quantum computer} of
sufficient capacity to apply Shor's algorithm to
numbers of relevant size.\footnote{%
Required qbits for attacks on RSA, DSA and ECDSA using key with a bit length n: \\
\vskip +1 pt
\begin{tabular}{|c|l|}
\hline
   RSA		&  2n + 3 \\
   DSA		&  2n + 3 \\
   ECDSA $2^n$	&  \~{}2n + 8 log n \\
   ECDSA p	&  \~{}4n \\
\hline
\end{tabular}
\vskip +6 pt
Please compare chapter 5.3 in
``SicAri -- Eine Sicherheitsplattform und deren Werkzeuge
f\"ur die ubiquit\"are Internetnutzung, KB2.1 -- Abschlussbericht,
\"Ubersicht \"uber Angriffe auf relevante kryptographische Verfahren'',
version 1.0, Mai 17, 2005,
Prof. Dr. Johannes Buchmann et al., TUD-KryptC and cv cryptovision GmbH
(\href{http://www.cdc.informatik.tu-darmstadt.de/~schepers/kb\_21\_angriffe.pdf}
 {\tt http://www.cdc.informatik.tu-darmstadt.de/\~{}schepers/kb\_21\_angriffe.pdf}) and the dissertation of Axel Schmidt at the same faculty.
}
Recent announcements of
significant progress in this area made by the
start-up company D-Wave (\url{www.dwavesys.com})
have been greeted with a lot of scepticism, even
ridicule.

The development of attacks on another frequently
used scheme called DSA (Digital Signature
Algorithm) and the Elliptic Curve Cryptography
(ECC) class of schemes moves in analogy to those
on RSA.  The security of these schemes depends on
the difficulty of computing discrete logarithms.
Even today, there is significant algorithmic
progress.  Quantum computers would render these
schemes insecure.

What's the state of affairs with the so-called
secret-key encryption schemes?  In 1977, DES was
introduced as the Data Encryption
Standard~\cite{DES-Standard:1977}.  Twenty-one
years later, the Electronic Frontier Foundation
(EFF) built the special purpose machine Deep Crack
which needed just 56 hours to break a DES key.
The problem with DES was that it used keys which
were too short.  It seems that the inventors of
DES did not foresee the speed of hardware
development.  The Advanced Encryption Standard
AES~\cite{AES-Standard:2002}, successor to DES, is
deemed secure at the moment even though there are
interesting, if still inefficient, methods to
attack AES with algebraic methods.


\section{Preparation for tomorrow}
\label{sec:preparations}

Is the security of today's cryptography measuring
up to its increasing importance?  The experience
shows: Carefully designed and implemented
cryptographic schemes have a life time of five to
twenty years.  Whoever uses RSA, ECC or AES for
short-term protection of data may feel safe.
Moreover, it is also possible to achieve long-term
authenticity, integrity and non-reputability of
data, e.g., using the multiple signature scheme
suggested by S\"onke Maseberg~\cite{maseberg-thesis:2002}.

However, current schemes cannot guarantee
long-term confidentiality.  And what is to be done in
twenty years from now?  What should we do if, quasi
over-night, unexpected mathematical progress
renders an important cryptographic scheme
insecure?  Three things are necessary to prepare
us for this event:

\begin{itemize}
\item a pool of secure alternative cryptographic schemes,
\item infrastructures that enable us to exchange
   one cryptographic scheme for another, easily
   and quickly, and
\item methods that ensure long-term confidentiality.
\end{itemize}

For many years, the cryptography group at the
Technische Universit\"at Darmstadt and its spin-off,
the company FlexSecure (\url{www.flexsecure.de}),
have worked to provide these tools.  The
trust center software FlexiTrust which is employed
by the German National Root Certification
Authority and the German Country Signing Authority
offers an infrastructure within which
cryptographic schemes can be easily exchanged.
The open source library FlexiProvider\index{FlexiProvider} implements a
multitude of cryptographic schemes.  Lately, we
have intensified our research into ``Post Quantum
Cryptography''\index{cryptography!post quantum}\index{post-quantum computing}\index{PQC} seeking
cryptographic schemes which
remain secure even in the event that powerful
quantum computers are built.

The security of public key cryptography
traditionally rests on the difficulty of the
solution of certain mathematical problems.  Today,
the following alternatives to the factorization and discrete
logarithm problems are discussed in depth:  the
decoding problem, the shortest and closest vector
problem in lattices, and the problem of solving
large systems of multivariate quadratic
equations.  It is conjectured that quantum
computers\index{quantum computer} offer little advantage if we try to
solve these problems efficiently.


\section{New mathematical problems}
\label{sec:problems}

Let us look at these alternatives a little more
closely.  The first encryption scheme based on the
decoding problem was proposed by
McEliece~\cite{mceliece:1978}\index{encryption!McEliece}.  The background:
Error-correcting codes are used to transmit or
store electronic data in such a way that they
remain undistorted even if a small number of bits
are changed in transit or on the storage media.
This property is used in, e.g., compact discs
(CDs).  The data on a CD can be reconstructed even
if the disc has been slightly scratched.

In a code-based encryption\index{encryption!code-based} scheme a message is
encrypted by adding a fixed number of errors to
(i.e. flipping a fixed numbers of bits of) the
encoded message.  Decoding requires knowledge of a
suitable decoding procedure which eliminates these
errors efficiently.  This method is the secret
key.  Code-based encryption is in general very
efficient.  At the moment, research focus on the
question which codes lead to secure encryption
schemes with keys which are as small as possible.

Encryption on the basis of lattice problems\index{encryption!lattice problems}
is very similar to that on the basis of
error-correcting codes.  Lattices are regular
structures of points in space.  For instance, the
points where the lines on squared paper cross form
a two-dimensional lattice.  For cryptographic
usage, the dimension of the lattices is chosen to
be much larger.  Encryption works as follows: The
plain-text is used to construct a lattice point
which is then slightly distorted in such a way
that it is no longer a lattice point, but close to
one.  Whoever knows a secret about the lattice is
able to find this lattice point in the vicinity of
the given point in space.  The lattice point in
turn yields the plain text.  A particularly
efficient lattice based encryption scheme is NTRU
Encrypt (\url{www.ntru.com})\index{encryption!lattice problems!NTR}.
However, because
NTRU was introduced fairly recently (in 1998), and
its specification underwent several changes due to
a variety of attacks, more cryptanalytic scrutiny
is required to achieve confidence in its security.

\section{New signatures}
\label{sec:signatures}

In 1979, Ralph Merkle proposed a remarkable
framework for new signature schemes in his PhD
thesis~\cite{merkle-thesis:1979}.
Contrary to all other signature schemes\index{signature!Merkle}, its
security does not rest on the difficulty of a
number-theoretic, algebraic or geometric problem.
The only thing it requires is something which
other signature schemes need anyway: a
cryptographically secure hash function and a
secure pseudo-random number generator.  Each new
hash function leads to a new signature algorithm.
In consequence, the Merkle scheme has the
potential to solve the problem of long-term
availability of digital signature schemes.

Merkle uses in his construction so-called One-Time
Signatures: Each new signature requires a new
signing key and a new verification key.  The idea
Merkle had was to reduce the validity of many
verification keys using a hash tree to the
validity of a unique public hash value.  When
generating keys for the Merkle scheme one has to
determine the number of signatures one can make
with it in advance.  For a long time this seemed a
significant disadvantage.  In
\cite{buchmann/coronado/dahmen/doering/klintsevich:2006},
however, a variant of Merkle's scheme was proposed
which allows to compute $2^{40}$ signatures with a
single key pair.


%\subsection{Quantum cryptography -- a loophole?}
\section{Quantum cryptography -- a way out of the impasse?}
\label{sec:quantum cryptography}\index{quantum cryptography}

From the point of view of today's state of the art
of cryptography, the problem of long-term
confidentiality remains unsolved: There is
\emph{no} practical method to protect the
confidentiality of an encrypted message over a
very long period of time.

One way out of that dilemma may be to employ
quantum cryptography: it allows for key agreement
schemes (of very long keys for one-time pads)
whose security is
guaranteed by the laws of quantum mechanics,
cf., e.g., \cite{bennett/brassard:1984b}.  At the
moment, however, quantum cryptography is still
rather inefficient, and it is unclear which
cryptographic functionalities can be implemented
on top of it.


\section{Conclusion}
\label{sec:conclusion}

What's on the balance sheet of today's cryptography?  We
have good tools to ensure short and medium term
security.  Software developers can employ these
tools in their applications with good conscience
as long as they make sure that components
can quickly be exchanged when they become
insecure.

In order to guarantee IT security for the future, too,
we need to prepare a portfolio of secure cryptographic
schemes.  This portfolio needs to contain schemes
which are suitable for the world of ubiquitous
computing with many less powerful computers.  It
also needs to contain schemes which remain secure
in the event that powerful quantum computers\index{quantum computer} are
built.  Several promising candidates have been
discussed in this article.  They need to be
studied carefully and prepared for use in everyday
scenarios.  The question how to ensure long-term
confidentiality remains an important open research
problem upon which cryptographic research should
focus.

\putbib
\addcontentsline{toc}{section}{Bibliography}   % To add it in Contents and PDF-Navigation

