% ..........................................................................
% --------------------------------------------------------------------------
% ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%              Krypto 2020
% /~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newpage
\hypertarget{Chapter_Crypto2020}{}
\chapter{Krypto 2020 --- Perspektiven für langfristige kryptographische
         Sicherheit}\index{Sicherheit!langfristige}
\label{Chapter_Crypto2020}
\begin{sloppypar}
(Johannes Buchmann, Erik Dahmen, Alexander May und Ulrich Vollmer,
TU Darmstadt, Mai~2007)\\
\end{sloppypar}

\hyphenation{mathe-ma-tischer Schwie-rig-keit}
% \begin{abstract}
%    Krypto-Schlüsseln und -Algorithmen droht Gefahr durch die
%    fortschreitende Entwicklung leistungsfähiger Hardware und neuer
%    mathematischer Verfahren. Wie lange können die heutigen Verfahren
%    noch halten, was sie versprechen? Und welche Alternativen zeichnen
%    sich für danach ab?
% \end{abstract}

Kryptographie ist ein Grundbaustein aller IT-Sicherheitslösungen.  Aber
wie lange sind die heute benutzten kryptographischen Verfahren noch
sicher? Reicht diese Zeit, um zum Beispiel medizinische Daten lang genug
geheim zu halten? Doch auch kurzfristig ließe sich großer Schaden
anrichten, wenn auch nur bestimmte Schlüssel gebrochen würden: Etwa bei
den digitalen Signaturen, welche die Authentizität von automatischen
Windows-Updates sichern.


\section{Verbreitete Verfahren}
\label{sec:verfahren}

In ihrer berühmten Arbeit aus dem Jahr 1978 stellten Rivest, Shamir und
Adleman \cite{rivest/shamir/adleman:1978} das RSA\index{RSA}
Public-Key-Verschlüsselungs- und Signaturverfahren vor. RSA ist auch
heute noch das in der Praxis meistverwendete Public-Key-System. Die
Sicherheit von RSA beruht auf der Schwierigkeit, so genannte RSA-Moduln
$N=pq$ in ihre (großen) Primfaktoren $p$ und $q$ zu zerlegen. In ihrer
Arbeit schlugen die Erfinder von RSA damals vor, für langfristige
Sicherheit $200$-stellige RSA-Moduln zu verwenden. Später
veröffentlichte die Firma RSA Security eine Liste von RSA-Moduln
wachsender Größe (RSA Challenge Numbers) und setzte Preise von insgesamt
635.000 US-\$ für das Faktorisieren dieser Zahlen aus (siehe
\url{www.rsasecurity.com/rsalabs/}).

Im Jahr 2005, also 27 Jahre nach der Erfindung von RSA, gelang es Bahr,
Boehm, Franke und Kleinjung von der Universität Bonn bereits innerhalb
von fünf Monaten eine 200-stellige RSA-Challenge zu faktorisieren und
damit die ursprüngliche Empfehlung langfristiger Sicherheit zu brechen
(\url{www.mat.uniroma2.it/~eal/rsa640.txt}).  Dies belegt anschaulich
die Fortschritte den letzten 30 Jahre bei der Faktorisierung von
RSA-Moduln. Diese beruhen sowohl auf bahnbrechenden mathematischen
Ideen, zum Beispiel der Entwicklung des Zahlkörpersiebs durch John Pollard,
als auch auf bedeutenden Fortschritten in der Computer- und
Implementierungstechnik.\footnote{%
Vergleiche Kapitel \ref{SecurityRSA}
\hyperlink{SecurityRSA}{Zur Sicherheit des RSA-Verfahrens}, und
speziell die Kapitel \ref{nt:NoteFactorization} und \ref{FactorisationResearch}.
}

Lenstra und Verheul\index{Lenstra/Verheul} entwickelten 2000 eine
Interpolationsformel zur Voraussage der Sicherheit\index{Sicherheit!Voraussage}
von RSA und anderen wichtigen kryptographischen Verfahren
(vgl.\ \url{www.keylength.com}). Dieser Formel zufolge muss man
derzeit schon 850-stellige RSA-Moduln verwenden,
um Sicherheit für die nächsten dreißig Jahre zu gewährleisten.

Aber auch eine solche Interpolationsformel ist keine
Sicherheitsgarantie! Brillante mathematische Ideen könnten jederzeit
dazu führen, dass das Lösen des Faktorisierungsproblems leicht
und RSA damit generell unbrauchbar wird. So bewies beispielsweise Peter
Shor 1996, dass ein Quantencomputer -- ein neuer Computertyp, der die
Gesetze der Quantenmechanik ausnutzt -- große Zahlen im Prinzip sehr
schnell faktorisieren könnte \cite{shor:1997}. Trotz intensiver
Forschungsbemühungen ist es aber auch heute noch unklar, ob sich jemals
hinreichend leistungsfähige Quantencomputer\index{Quantencomputer}
bauen lassen.\footnote{%
Benötigte qbits für Angriffe auf RSA, DSA und ECDSA für Schlüssel
der Bit-Länge n: \\
\vskip +1 pt
\begin{tabular}{|c|l|}
\hline
   RSA		&  2n + 3 \\
   DSA		&  2n + 3 \\
   ECDSA $2^n$	&  \~{}2n + 8 log n \\
   ECDSA p	&  \~{}4n \\
\hline
\end{tabular}
\vskip +6 pt
Vergleiche Kap. 5.3 in
\glqq SicAri -- Eine Sicherheitsplattform und deren Werkzeuge
für die ubiquitäre Internetnutzung, KB2.1 -- Abschlussbericht,
Übersicht über Angriffe auf relevante kryptographische Verfahren\grqq,
Version 1.0, 17. Mai 2005,
Prof. Dr. Johannes Buchmann et al., TUD-KryptC und cv cryptovision GmbH
(\href{http://www.cdc.informatik.tu-darmstadt.de/~schepers/kb\_21\_angriffe.pdf}
 {\tt http://www.cdc.informatik.tu-darmstadt.de/\~{}schepers/kb\_21\_angriffe.pdf})
und die Dissertation von Axel Schmidt am gleichen Lehrstuhl.
% http://www.cdc.informatik.tu-darmstadt.de/mitarbeiter/axel.html
}
Aktuelle Verlautbarungen der Start-up-Firma D-Wave (\url{www.dwavesys.com})
trafen auf verbreitete Skepsis, ja Spott.

Analog zu RSA verläuft die Entwicklung bei Angriffen
auf die meistverwendeten Public-Key-Alternativen:
den Digital Signature Algorithm (DSA) und Elliptic Curve
Cryptography (ECC), die beide auf der Schwierigkeit
der Berechnung diskreter Logarithmen beruhen. Es gibt
schon heute deutliche algorithmische Fortschritte und
Quantencomputer würden auch diese Verfahren unsicher
machen.

Wie steht es um die langfristige Sicherheit von so genannten
Secret-Key-Verschlüsselungsver"-fah"-ren? DES wurde 1977 als Data Encryption
Standard eingeführt \cite{DES-Standard:1977} -- 21 Jahre später stellte
die Electronic Frontier Foundation (EFF) den Spezialcomputer Deep Crack
vor, der DES in 56 Stunden bricht. Das Problem von DES war die zu kurz
gewählte Schlüssellänge: Offenbar hatten seine Erfinder die rasante
Entwicklung bei der Hardware nicht richtig einkalkuliert. Der
DES-Nachfolger Advanced Encryption Standard (AES)
\cite{AES-Standard:2002} gilt heute als sicher, wenngleich interessante
Angriffsversuche mit algebraischen Methoden existieren.


\section{Vorsorge für morgen}
\label{vorsorge}

Ist die heutige Kryptographie angesichts ihrer
wachsenden Bedeutung noch sicher genug? Die Erfahrung
zeigt: Sorgfältig konzipierte und implementierte kryptographische
Verfahren haben eine Lebensdauer von 5 bis
20 Jahren. Wer heute RSA, ECC oder AES zur kurzfristigen
Absicherung verwendet, darf sich sicher fühlen. Und
langfristige Verbindlichkeit lässt sich beispielsweise mit
den von Jan Sönke Maseberg vorgeschlagenen multiplen
Signaturen lösen \cite{maseberg-thesis:2002}.

Aber langfristige Vertraulichkeit können wir
heute mit den genannten Verfahren nicht garan"-tieren.
Und was ist in 20 Jahren? Was kann man tun, wenn ein
unerwarteter mathematischer Fortschritt ein wichtiges
Kryptoverfahren plötzlich -- quasi über Nacht -- unsicher
macht? Drei Dinge sind zur Vorbereitung nötig:

\begin{itemize}
\item ein Pool alternativer sicherer Kryptoverfahren,
\item Infrastrukturen, die es ermöglichen, unsichere kryptographische
   Verfahren leicht gegen sichere auszutauschen und
\item Verfahren, die langfristige Vertraulichkeit sicherstellen.
\end{itemize}

Diese Ziele verfolgen auch die Kryptoarbeitsgruppe der TU Darmstadt und
der daraus entstandene Spin-off FlexSecure (\url{www.flexsecure.de})
seit vielen Jahren. Die Trustcenter-Software FlexiTrust, die in der
deutschen nationalen Root-Zertifizierungsstelle und in der deutschen
Country-Signing-Zertifizierungsstelle verwendet wird, ist eine
Infrastruktur, die den einfachen Austausch von Kryptographie-Verfahren möglich
macht. Die Open-Source-Bibliothek FlexiPro"-vider\index{FlexiProvider}
(siehe \url{www.flexiprovider.de}) stellt zudem eine Vielzahl
unterschiedlicher kryptographischer Verfahren zur Verfügung und in
jüngerer Zeit laufen intensive Forschungen zur \glqq Post Quantum
Cryptography\grqq\index{Kryptographie!Post Quantum}: Kryptographie,
die auch dann noch sicher bleibt, wenn es
(leistungsfähige) Quantencomputer tatsächlich gibt.

Die Sicherheit der Public-Key-Kryptographie beruht traditionell auf der
Schwierigkeit, bestimmte mathematische Probleme zu lösen. Heute
diskutierte Alternativen zum Faktorisierungs- und Diskrete-Logarithmen-
Problem sind: das Dekodierungsproblem, das Problem, kürzeste und näch"-ste
Gittervektoren zu berechnen, und das Problem, quadratische Gleichungen
mit vielen Variablen zu lösen. Es wird vermutet, dass diese Probleme
auch von Quantencomputern\index{Quantencomputer} nicht effizient lösbar wären.


\section{Neue mathematische Probleme}
\label{sec:probleme}

Wie sehen diese Alternativen näher aus? Verschlüsselung
auf der Basis des Dekodierungsproblems wurde
von McEliece erfunden~\cite{mceliece:1978}\index{Verschlüsselung!McEliece}.
Der Hintergrund: Fehlerkorrigierende
Codes dienen dazu, digitale Informationen so zu
speichern, dass sie selbst dann noch vollständig lesbar
bleiben, wenn einzelne Bits auf dem Speichermedium
verändert werden. Diese Eigenschaft nutzen zum Beispiel
CDs, sodass Informationen auf leicht verkratzten Datenträgern
immer noch vollständig rekonstruierbar sind.

Bei der codebasierten Verschlüsselung\index{Verschlüsselung!codebasiert}
werden Daten verschlüsselt, indem
zu ihrer Kodierung mit einem öffentlich bekannten Code gezielt Fehler
addiert werden, das heißt einzelne Bits werden verändert. Die
Entschlüsselung erfordert die Kenntnis einer geeigneten
Dekodierungsmethode, welche diese Fehler effizient zu entfernen vermag.
Diese Methode ist der geheime Schlüssel -- nur wer sie kennt, kann
dechiffrieren. Codebasierte Public-Key-Verschlüsselung ist im
Allgemeinen sehr effizient durchführbar. Derzeit wird daran geforscht,
welche Codes zu sicheren Verschlüsselungsverfahren mit möglichst kleinen
Schlüsseln führen.

Verschlüsselung auf der Grundlage von
Gitterproblemen\index{Verschlüsselung!Gitterprobleme} ist den
codebasierten Verschlüssel"-ungs"-ver"-fah"-ren sehr ähnlich. Gitter sind
regelmäßige Strukturen von Punkten im Raum. Zum Beispiel bilden die
Eckpunkte der Quadrate auf kariertem Papier ein zweidimensionales
Gitter. In der Kryptographie verwendet man jedoch Gitter in viel höheren
Dimensionen. Verschlüsselt wird nach dem folgenden Prinzip: Aus dem
Klartext wird zunächst ein Gitterpunkt konstruiert und anschließend
geringfügig verschoben, sodass er kein Gitterpunkt mehr ist, aber in der
Nähe eines solchen liegt. Wer ein Geheimnis über das Gitter kennt, kann
diesen Gitterpunkt in der Nähe finden und damit entschlüsseln. Ein
besonders praktikables Gitterverschlüsselungsverfahren ist NTRU
(\url{www.ntru.com})\index{Verschlüsselung!Gitterprobleme!NTRU}.
Da NTRU vor vergleichsweise kurzer Zeit
eingeführt wurde (1998) und seine Spezifizierung aufgrund verschiedener
Angriffe mehrfach geändert wurde, sind allerdings noch weitere
kryptanalytische Untersuchungen erforderlich, um Vertrauen in dieses
Verfahren zu gewinnen.


\section{Neue Signaturen}
\label{sec:signaturen}

1979 schlug Ralph Merkle einen bemerkenswerten Ansatz für die
Konstruktion von Signaturverfahren in seiner Dissertation
\cite{merkle-thesis:1979} vor.  Im Gegensatz zu allen anderen
Signaturverfahren\index{Signatur!Merkle} beruht seine Sicherheit
nicht darauf, dass ein
zahlentheoretisches, algebraisches oder geometrisches Problem schwer
lösbar ist. Benötigt wird ausschließlich, was andere Signaturverfahren
ebenfalls voraussetzen: eine sichere kryptographische Hash-Funktion und
ein sicherer Pseudozufallszahlengenerator. Jede neue Hash-Funktion führt
somit zu einem neuen Signaturalgorithmus, wodurch das Merkle-Verfahren
das Potential hat, das Problem der langfristigen Verfügbarkeit digitaler
Signaturverfahren zu lösen.

Merkle verwendet in seiner Konstruktion so genannte Einmal-Signaturen:
Dabei benötigt jede neue Signatur einen neuen Signierschlüssel und einen
neuen Verifikationsschlüssel. Die Idee von Merkle ist es, die Gültigkeit
vieler Verifikationsschlüssel mittels eines Hash-Baums auf die
Gültigkeit eines einzelnen öffentlichen Schlüssels zurückzuführen. Im
Merkle-Verfahren muss man bei der Schlüsselerzeugung eine Maximalzahl
möglicher Signaturen festlegen, was lange wie ein gravierender Nachteil
aussah. In \cite{buchmann/coronado/dahmen/doering/klintsevich:2006}
wurde jedoch eine Variante des Merkle-Verfahrens vorgestellt, die es
ermöglicht, äußerst effizient mit einem einzigen Schlüsselpaar bis zu
$2^{40}$ Signaturen zu erzeugen und zu verifizieren.


\section{Quantenkryptographie -- Ein Ausweg?}
\label{sec:Quantenkryptographie}\index{Quantenkryptographie}

Ungelöst bleibt aus Sicht der heutigen Kryptographie das Problem der
langfristigen Vertraulich"-keit: Ein praktikables Verfahren, das die
Vertraulichkeit einer verschlüsselten Nachricht über einen sehr langen
Zeitraum sicherstellt, ist derzeit nicht bekannt.

Einen Ausweg kann hier möglicherweise die Quantenkryptographie liefern:
Sie ermöglicht Verfahren zum Schlüsselaustausch (sehr lange Schlüssel
für One-time-Pads), deren Sicherheit auf der Gültigkeit
der Gesetze der Quantenmechanik beruhen, vgl.\ z.B.\
\cite{bennett/brassard:1984b}. Jedoch sind bekannte Verfahren der
Quantenkryptographie derzeit noch sehr ineffizient und es ist unklar,
welche kryptographischen Funktionen damit realisiert werden können.


\section{Fazit}
\label{sec:fazit}

Wie lautet die Bilanz? Die heutige Kryptographie
liefert gute Werkzeuge, um kurzfristige und mittelfristige
Sicherheit zu gewährleisten. Anwendungen können diese
Werkzeuge ruhigen Gewissens verwenden, solange sie in
der Lage sind, unsichere Komponenten schnell gegen
Alternativen auszutauschen. 

Um IT-Sicherheit auch für die Zukunft zu garantieren, müssen wir ein
Portfolio sicherer kryptographischer Funktionen vorbereiten. Es benötigt
Verfahren, die sowohl für die Welt der allgegenwärtigen (weniger
leistungsfähigen) Computer geeignet sind als auch dann sicher bleiben,
wenn es leistungsfähige Quantencomputer\index{Quantencomputer} gibt.
Einige viel versprechende
Kandidaten für ein solches Portfolio wurden in diesem Artikel
vorgestellt; diese müssen jedoch noch sorgfältig erforscht und für die
Praxis aufbereitet werden. Die Frage nach einem Verfahren zur Sicherung
langfristiger Vertraulichkeit bleibt ein wichtiges offenes Problem, auf
das sich zukünftige Forschung fokussieren sollte.

\putbib
\addcontentsline{toc}{section}{Literaturverzeichnis}

